/**
 * AUTO-GENERATED FILE - DO NOT EDIT
 * Generated by: scripts/generate-skills.ts
 * Run: bun run scripts/generate-skills.ts
 */

import type { SkillDefinition } from './types.js';

/**
 * List of builtin skill names.
 */
export const BUILTIN_SKILL_NAMES = ["agents-md-mastery", "ast-grep", "brainstorming", "code-reviewer", "dispatching-parallel-agents", "docker-mastery", "executing-plans", "finishing-a-development-branch", "parallel-exploration", "receiving-code-review", "requesting-code-review", "subagent-driven-development", "systematic-debugging", "test-driven-development", "using-git-worktrees", "verification-before-completion", "warcraft", "writing-plans", "writing-skills"] as const;

/**
 * All builtin skill definitions.
 */
export const BUILTIN_SKILLS: SkillDefinition[] = [
  {
    name: "agents-md-mastery",
    description: "Use when bootstrapping, updating, or reviewing AGENTS.md — teaches what makes effective agent memory, how to structure sections, signal vs noise filtering, and when to prune stale entries",
    template: "# AGENTS.md Mastery\n\n## Overview\n\n**AGENTS.md is pseudo-memory loaded at session start.** Every line shapes agent behavior for the entire session. Quality beats quantity. Write for agents, not humans.\n\nUnlike code comments or READMEs, AGENTS.md entries persist across all agent sessions. A bad entry misleads agents hundreds of times. A missing entry causes the same mistake repeatedly.\n\n**Core principle:** Optimize for agent comprehension and behavioral change, not human readability.\n\n## The Iron Law\n\n```\nEVERY ENTRY MUST CHANGE AGENT BEHAVIOR\n```\n\nIf an entry doesn't:\n- Prevent a specific mistake\n- Enable a capability the agent would otherwise miss\n- Override a default assumption that breaks in this codebase\n\n...then it doesn't belong in AGENTS.md.\n\n**Test:** Would a fresh agent session make a mistake without this entry? If no → noise.\n\n## When to Use\n\n| Trigger | Action |\n|---------|--------|\n| New project bootstrap | Write initial AGENTS.md with build/test/style basics |\n| Feature completion | Sync new learnings via `warcraft_agents_md` tool |\n| Periodic review | Audit for stale/redundant entries (quarterly) |\n| Quality issues | Agent repeating mistakes? Check if AGENTS.md has the fix |\n\n## What Makes Good Agent Memory\n\n### Signal Entries (Keep)\n\n✅ **Project-specific conventions:**\n- \"We use Zustand, not Redux — never add Redux\"\n- \"Auth lives in `/lib/auth` — never create auth elsewhere\"\n- \"Run `bun test` not `npm test` (we don't use npm)\"\n\n✅ **Non-obvious patterns:**\n- \"Use `.js` extension for local imports (ESM requirement)\"\n- \"Worktrees don't share `node_modules` — run `bun install` in each\"\n- \"SandboxConfig is in `dockerSandboxService.ts`, NOT `types.ts`\"\n\n✅ **Gotchas that break builds:**\n- \"Never use `ensureDirSync` — doesn't exist. Use `ensureDir` (sync despite name)\"\n- \"Import from `../utils/paths.js` not `./paths` (ESM strict)\"\n\n### Noise Entries (Remove)\n\n❌ **Agent already knows:**\n- \"This project uses TypeScript\" (agent detects from files)\n- \"We follow semantic versioning\" (universal convention)\n- \"Use descriptive variable names\" (generic advice)\n\n❌ **Irrelevant metadata:**\n- \"Created on January 2024\"\n- \"Originally written by X\"\n- \"License: MIT\" (in LICENSE file already)\n\n❌ **Describes what code does:**\n- \"FeatureService manages features\" (agent can read code)\n- \"The system uses git worktrees\" (observable from commands)\n\n### Rule of Thumb\n\n**Signal:** Changes how agent acts  \n**Noise:** Documents what agent observes\n\n## Section Structure for Fast Comprehension\n\nAgents read AGENTS.md top-to-bottom once at session start. Put high-value info first:\n\n```markdown\n# Project Name\n\n## Build & Test Commands\n# ← Agents need this IMMEDIATELY\nbun run build\nbun run test\nbun run release:check\n\n## Code Style\n# ← Prevents syntax/import errors\n- Semicolons: Yes\n- Quotes: Single\n- Imports: Use `.js` extension\n\n## Architecture\n# ← Key directories, where things live\npackages/\n├── warcraft-core/      # Shared logic\n├── opencode-warcraft/  # Plugin\n\n## Important Patterns\n# ← How to do common tasks correctly\nUse `readText` from paths.ts, not fs.readFileSync\n\n## Gotchas & Anti-Patterns\n# ← Things that break or mislead\nNEVER use `ensureDirSync` — doesn't exist\n```\n\n**Keep total under 500 lines.** Beyond that, agents lose focus and miss critical entries.\n\n## The Sync Workflow\n\nAfter completing a feature, sync learnings to AGENTS.md:\n\n1. **Trigger sync:**\n   ```typescript\n   warcraft_agents_md({ action: 'sync', feature: 'feature-name' })\n   ```\n\n2. **Review each proposal:**\n   - Read the proposed change\n   - Ask: \"Does this change agent behavior?\"\n   - Check: Is this already obvious from code/files?\n\n3. **Accept signal, reject noise:**\n   - ❌ \"TypeScript is used\" → Agent detects this\n   - ✅ \"Use `.js` extension for imports\" → Prevents build failures\n\n4. **Apply approved changes:**\n   ```typescript\n   warcraft_agents_md({ action: 'apply' })\n   ```\n\n**Warning:** Don't auto-approve all proposals. One bad entry pollutes all future sessions.\n\n## When to Prune\n\nRemove entries when they become:\n\n**Outdated:**\n- \"We use Redux\" → Project migrated to Zustand\n- \"Node 16 compatibility required\" → Now on Node 22\n\n**Redundant:**\n- \"Use single quotes\" + \"Strings use single quotes\" → Keep one\n- Near-duplicates in different sections\n\n**Too generic:**\n- \"Write clear code\" → Applies to any project\n- \"Test your changes\" → Universal advice\n\n**Describing code:**\n- \"TaskService manages tasks\" → Agent can read `TaskService` class\n- \"Worktrees are in `.beads/artifacts/.worktrees/`\" → Observable from filesystem\n\n**Proven unnecessary:**\n- Entry added 6 months ago, but agents haven't hit that issue since\n\n## Red Flags\n\n| Warning Sign | Why It's Bad | Fix |\n|-------------|-------------|-----|\n| AGENTS.md > 800 lines | Agents lose focus, miss critical info | Prune aggressively |\n| Describes what code does | Agent can read code | Remove descriptions |\n| Missing build/test commands | First thing agents need | Add at top |\n| No gotchas section | Agents repeat past mistakes | Document failure modes |\n| Generic best practices | Doesn't change behavior | Remove or make specific |\n| Outdated patterns | Misleads agents | Prune during sync |\n\n## Anti-Patterns\n\n| Anti-Pattern | Better Approach |\n|-------------|----------------|\n| \"Document everything\" | Document only what changes behavior |\n| \"Keep for historical record\" | Version control is history |\n| \"Might be useful someday\" | Add when proven necessary |\n| \"Explains the system\" | Agents read code for that |\n| \"Comprehensive reference\" | AGENTS.md is a filter, not docs |\n\n## Good Examples\n\n**Build Commands (High value, agents need immediately):**\n```markdown\n## Build & Test Commands\nbun run build              # Build all packages\nbun run test               # Run all tests\nbun run release:check      # Full CI check\n```\n\n**Project-Specific Convention (Prevents mistakes):**\n```markdown\n## Code Style\n- Imports: Use `.js` extension for local imports (ESM requirement)\n- Paths: Import from `../utils/paths.js` never `./paths`\n```\n\n**Non-Obvious Gotcha (Prevents build failure):**\n```markdown\n## Important Patterns\nUse `ensureDir` from paths.ts — sync despite name\nNEVER use `ensureDirSync` (doesn't exist)\n```\n\n## Bad Examples\n\n**Generic advice (agent already knows):**\n```markdown\n## Best Practices\n- Use meaningful variable names\n- Write unit tests\n- Follow DRY principle\n```\n\n**Describes code (agent can read it):**\n```markdown\n## Architecture\nThe FeatureService class manages features. It has methods\nfor create, read, update, and delete operations.\n```\n\n**Irrelevant metadata:**\n```markdown\n## Project History\nCreated in January 2024 by the platform team.\nOriginally built for internal use.\n```\n\n## Verification\n\nBefore finalizing AGENTS.md updates:\n\n- [ ] Every entry answers: \"What mistake does this prevent?\"\n- [ ] No generic advice that applies to all projects\n- [ ] Build/test commands are first\n- [ ] Gotchas section exists and is populated\n- [ ] Total length under 500 lines (800 absolute max)\n- [ ] No entries describing what code does\n- [ ] Fresh agent session would benefit from each entry\n\n## Summary\n\nAGENTS.md is **behavioral memory**, not documentation:\n- Write for agents, optimize for behavior change\n- Signal = prevents mistakes, Noise = describes observables\n- Sync after features, prune quarterly\n- Test: Would agent make a mistake without this entry?\n\n**Quality > quantity. Every line counts.**",
  },
  {
    name: "ast-grep",
    description: "Guide for writing ast-grep rules to perform structural code search and analysis. Use when users need to search codebases using Abstract Syntax Tree (AST) patterns, find specific code structures, or perform complex code queries that go beyond simple text search. This skill should be used when users ask to search for code patterns, find specific language constructs, or locate code with particular structural characteristics.",
    template: "# ast-grep Code Search\n\n## Overview\n\nThis skill helps translate natural language queries into ast-grep rules for structural code search. ast-grep uses Abstract Syntax Tree (AST) patterns to match code based on its structure rather than just text, enabling powerful and precise code search across large codebases.\n\n## When to Use This Skill\n\nUse this skill when users:\n- Need to search for code patterns using structural matching (e.g., \"find all async functions that don't have error handling\")\n- Want to locate specific language constructs (e.g., \"find all function calls with specific parameters\")\n- Request searches that require understanding code structure rather than just text\n- Ask to search for code with particular AST characteristics\n- Need to perform complex code queries that traditional text search cannot handle\n\n## General Workflow\n\nFollow this process to help users write effective ast-grep rules:\n\n### Step 1: Understand the Query\n\nClearly understand what the user wants to find. Ask clarifying questions if needed:\n- What specific code pattern or structure are they looking for?\n- Which programming language?\n- Are there specific edge cases or variations to consider?\n- What should be included or excluded from matches?\n\n### Step 2: Create Example Code\n\nWrite a simple code snippet that represents what the user wants to match. Save this to a temporary file for testing.\n\n**Example:**\nIf searching for \"async functions that use await\", create a test file:\n\n```javascript\n// test_example.js\nasync function example() {\n  const result = await fetchData();\n  return result;\n}\n```\n\n### Step 3: Write the ast-grep Rule\n\nTranslate the pattern into an ast-grep rule. Start simple and add complexity as needed.\n\n**Key principles:**\n- Always use `stopBy: end` for relational rules (`inside`, `has`) to ensure search goes to the end of the direction\n- Use `pattern` for simple structures\n- Use `kind` with `has`/`inside` for complex structures\n- Break complex queries into smaller sub-rules using `all`, `any`, or `not`\n\n**Example rule file (test_rule.yml):**\n```yaml\nid: async-with-await\nlanguage: javascript\nrule:\n  kind: function_declaration\n  has:\n    pattern: await $EXPR\n    stopBy: end\n```\n\nSee `references/rule_reference.md` for comprehensive rule documentation.\n\n### Step 4: Test the Rule\n\nUse ast-grep CLI to verify the rule matches the example code. There are two main approaches:\n\n**Option A: Test with inline rules (for quick iterations)**\n```bash\necho \"async function test() { await fetch(); }\" | ast-grep scan --inline-rules \"id: test\nlanguage: javascript\nrule:\n  kind: function_declaration\n  has:\n    pattern: await \\$EXPR\n    stopBy: end\" --stdin\n```\n\n**Option B: Test with rule files (recommended for complex rules)**\n```bash\nast-grep scan --rule test_rule.yml test_example.js\n```\n\n**Debugging if no matches:**\n1. Simplify the rule (remove sub-rules)\n2. Add `stopBy: end` to relational rules if not present\n3. Use `--debug-query` to understand the AST structure (see below)\n4. Check if `kind` values are correct for the language\n\n### Step 5: Search the Codebase\n\nOnce the rule matches the example code correctly, search the actual codebase:\n\n**For simple pattern searches:**\n```bash\nast-grep run --pattern 'console.log($ARG)' --lang javascript /path/to/project\n```\n\n**For complex rule-based searches:**\n```bash\nast-grep scan --rule my_rule.yml /path/to/project\n```\n\n**For inline rules (without creating files):**\n```bash\nast-grep scan --inline-rules \"id: my-rule\nlanguage: javascript\nrule:\n  pattern: \\$PATTERN\" /path/to/project\n```\n\n## ast-grep CLI Commands\n\n### Inspect Code Structure (--debug-query)\n\nDump the AST structure to understand how code is parsed:\n\n```bash\nast-grep run --pattern 'async function example() { await fetch(); }' \\\n  --lang javascript \\\n  --debug-query=cst\n```\n\n**Available formats:**\n- `cst`: Concrete Syntax Tree (shows all nodes including punctuation)\n- `ast`: Abstract Syntax Tree (shows only named nodes)\n- `pattern`: Shows how ast-grep interprets your pattern\n\n**Use this to:**\n- Find the correct `kind` values for nodes\n- Understand the structure of code you want to match\n- Debug why patterns aren't matching\n\n**Example:**\n```bash\n# See the structure of your target code\nast-grep run --pattern 'class User { constructor() {} }' \\\n  --lang javascript \\\n  --debug-query=cst\n\n# See how ast-grep interprets your pattern\nast-grep run --pattern 'class $NAME { $$$BODY }' \\\n  --lang javascript \\\n  --debug-query=pattern\n```\n\n### Test Rules (scan with --stdin)\n\nTest a rule against code snippet without creating files:\n\n```bash\necho \"const x = await fetch();\" | ast-grep scan --inline-rules \"id: test\nlanguage: javascript\nrule:\n  pattern: await \\$EXPR\" --stdin\n```\n\n**Add --json for structured output:**\n```bash\necho \"const x = await fetch();\" | ast-grep scan --inline-rules \"...\" --stdin --json\n```\n\n### Search with Patterns (run)\n\nSimple pattern-based search for single AST node matches:\n\n```bash\n# Basic pattern search\nast-grep run --pattern 'console.log($ARG)' --lang javascript .\n\n# Search specific files\nast-grep run --pattern 'class $NAME' --lang python /path/to/project\n\n# JSON output for programmatic use\nast-grep run --pattern 'function $NAME($$$)' --lang javascript --json .\n```\n\n**When to use:**\n- Simple, single-node matches\n- Quick searches without complex logic\n- When you don't need relational rules (inside/has)\n\n### Search with Rules (scan)\n\nYAML rule-based search for complex structural queries:\n\n```bash\n# With rule file\nast-grep scan --rule my_rule.yml /path/to/project\n\n# With inline rules\nast-grep scan --inline-rules \"id: find-async\nlanguage: javascript\nrule:\n  kind: function_declaration\n  has:\n    pattern: await \\$EXPR\n    stopBy: end\" /path/to/project\n\n# JSON output\nast-grep scan --rule my_rule.yml --json /path/to/project\n```\n\n**When to use:**\n- Complex structural searches\n- Relational rules (inside, has, precedes, follows)\n- Composite logic (all, any, not)\n- When you need the power of full YAML rules\n\n**Tip:** For relational rules (inside/has), always add `stopBy: end` to ensure complete traversal.\n\n## Tips for Writing Effective Rules\n\n### Always Use stopBy: end\n\nFor relational rules, always use `stopBy: end` unless there's a specific reason not to:\n\n```yaml\nhas:\n  pattern: await $EXPR\n  stopBy: end\n```\n\nThis ensures the search traverses the entire subtree rather than stopping at the first non-matching node.\n\n### Start Simple, Then Add Complexity\n\nBegin with the simplest rule that could work:\n1. Try a `pattern` first\n2. If that doesn't work, try `kind` to match the node type\n3. Add relational rules (`has`, `inside`) as needed\n4. Combine with composite rules (`all`, `any`, `not`) for complex logic\n\n### Use the Right Rule Type\n\n- **Pattern**: For simple, direct code matching (e.g., `console.log($ARG)`)\n- **Kind + Relational**: For complex structures (e.g., \"function containing await\")\n- **Composite**: For logical combinations (e.g., \"function with await but not in try-catch\")\n\n### Debug with AST Inspection\n\nWhen rules don't match:\n1. Use `--debug-query=cst` to see the actual AST structure\n2. Check if metavariables are being detected correctly\n3. Verify the node `kind` matches what you expect\n4. Ensure relational rules are searching in the right direction\n\n### Escaping in Inline Rules\n\nWhen using `--inline-rules`, escape metavariables in shell commands:\n- Use `\\$VAR` instead of `$VAR` (shell interprets `$` as variable)\n- Or use single quotes: `'$VAR'` works in most shells\n\n**Example:**\n```bash\n# Correct: escaped $\nast-grep scan --inline-rules \"rule: {pattern: 'console.log(\\$ARG)'}\" .\n\n# Or use single quotes\nast-grep scan --inline-rules 'rule: {pattern: \"console.log($ARG)\"}' .\n```\n\n## Common Use Cases\n\n### Find Functions with Specific Content\n\nFind async functions that use await:\n```bash\nast-grep scan --inline-rules \"id: async-await\nlanguage: javascript\nrule:\n  all:\n    - kind: function_declaration\n    - has:\n        pattern: await \\$EXPR\n        stopBy: end\" /path/to/project\n```\n\n### Find Code Inside Specific Contexts\n\nFind console.log inside class methods:\n```bash\nast-grep scan --inline-rules \"id: console-in-class\nlanguage: javascript\nrule:\n  pattern: console.log(\\$\\$\\$)\n  inside:\n    kind: method_definition\n    stopBy: end\" /path/to/project\n```\n\n### Find Code Missing Expected Patterns\n\nFind async functions without try-catch:\n```bash\nast-grep scan --inline-rules \"id: async-no-trycatch\nlanguage: javascript\nrule:\n  all:\n    - kind: function_declaration\n    - has:\n        pattern: await \\$EXPR\n        stopBy: end\n    - not:\n        has:\n          pattern: try { \\$\\$\\$ } catch (\\$E) { \\$\\$\\$ }\n          stopBy: end\" /path/to/project\n```\n\n## Resources\n\n### references/\nContains detailed documentation for ast-grep rule syntax:\n- `rule_reference.md`: Comprehensive ast-grep rule documentation covering atomic rules, relational rules, composite rules, and metavariables\n\nLoad these references when detailed rule syntax information is needed.\n\n## Reference: rule_reference.md\n\n# ast-grep Rule Reference\n\nThis document provides comprehensive documentation for ast-grep rule syntax, covering all rule types and metavariables.\n\n## Introduction to ast-grep Rules\n\nast-grep rules are declarative specifications for matching and filtering Abstract Syntax Tree (AST) nodes. They enable structural code search and analysis by defining conditions an AST node must meet to be matched.\n\n### Rule Categories\n\nast-grep rules are categorized into three types:\n\n* **Atomic Rules**: Match individual AST nodes based on intrinsic properties like code patterns (`pattern`), node type (`kind`), or text content (`regex`).\n* **Relational Rules**: Define conditions based on a target node's position or relationship to other nodes (e.g., `inside`, `has`, `precedes`, `follows`).\n* **Composite Rules**: Combine other rules using logical operations (AND, OR, NOT) to form complex matching criteria (e.g., `all`, `any`, `not`, `matches`).\n\n## Anatomy of an ast-grep Rule Object\n\nThe ast-grep rule object is the core configuration unit defining how ast-grep identifies and filters AST nodes. It's typically written in YAML format.\n\n### General Structure\n\nEvery field within an ast-grep Rule Object is optional, but at least one \"positive\" key (e.g., `kind`, `pattern`) must be present.\n\nA node matches a rule if it satisfies all fields defined within that rule object, implying an implicit logical AND operation.\n\nFor rules using metavariables that depend on prior matching, explicit `all` composite rules are recommended to guarantee execution order.\n\n### Rule Object Properties\n\n| Property | Type | Category | Purpose | Example |\n| :--- | :--- | :--- | :--- | :--- |\n| `pattern` | String or Object | Atomic | Matches AST node by code pattern. | `pattern: console.log($ARG)` |\n| `kind` | String | Atomic | Matches AST node by its kind name. | `kind: call_expression` |\n| `regex` | String | Atomic | Matches node's text by Rust regex. | `regex: ^[a-z]+$` |\n| `nthChild` | number, string, Object | Atomic | Matches nodes by their index within parent's children. | `nthChild: 1` |\n| `range` | RangeObject | Atomic | Matches node by character-based start/end positions. | `range: { start: { line: 0, column: 0 }, end: { line: 0, column: 10 } }` |\n| `inside` | Object | Relational | Target node must be inside node matching sub-rule. | `inside: { pattern: class $C { $$$ }, stopBy: end }` |\n| `has` | Object | Relational | Target node must have descendant matching sub-rule. | `has: { pattern: await $EXPR, stopBy: end }` |\n| `precedes` | Object | Relational | Target node must appear before node matching sub-rule. | `precedes: { pattern: return $VAL }` |\n| `follows` | Object | Relational | Target node must appear after node matching sub-rule. | `follows: { pattern: import $M from '$P' }` |\n| `all` | Array<Rule> | Composite | Matches if all sub-rules match. | `all: [ { kind: call_expression }, { pattern: foo($A) } ]` |\n| `any` | Array<Rule> | Composite | Matches if any sub-rules match. | `any: [ { pattern: foo() }, { pattern: bar() } ]` |\n| `not` | Object | Composite | Matches if sub-rule does not match. | `not: { pattern: console.log($ARG) }` |\n| `matches` | String | Composite | Matches if predefined utility rule matches. | `matches: my-utility-rule-id` |\n\n## Atomic Rules\n\nAtomic rules match individual AST nodes based on their intrinsic properties.\n\n### pattern: String and Object Forms\n\nThe `pattern` rule matches a single AST node based on a code pattern.\n\n**String Pattern**: Directly matches using ast-grep's pattern syntax with metavariables.\n\n```yaml\npattern: console.log($ARG)\n```\n\n**Object Pattern**: Offers granular control for ambiguous patterns or specific contexts.\n\n* `selector`: Pinpoints a specific part of the parsed pattern to match.\n  ```yaml\n  pattern:\n    selector: field_definition\n    context: class { $F }\n  ```\n\n* `context`: Provides surrounding code context for correct parsing.\n\n* `strictness`: Modifies the pattern's matching algorithm (`cst`, `smart`, `ast`, `relaxed`, `signature`).\n  ```yaml\n  pattern:\n    context: foo($BAR)\n    strictness: relaxed\n  ```\n\n### kind: Matching by Node Type\n\nThe `kind` rule matches an AST node by its `tree_sitter_node_kind` name, derived from the language's Tree-sitter grammar. Useful for targeting constructs like `call_expression` or `function_declaration`.\n\n```yaml\nkind: call_expression\n```\n\n### regex: Text-Based Node Matching\n\nThe `regex` rule matches the entire text content of an AST node using a Rust regular expression. It's not a \"positive\" rule, meaning it matches any node whose text satisfies the regex, regardless of its structural kind.\n\n### nthChild: Positional Node Matching\n\nThe `nthChild` rule finds nodes by their 1-based index within their parent's children list, counting only named nodes by default.\n\n* `number`: Matches the exact nth child. Example: `nthChild: 1`\n* `string`: Matches positions using An+B formula. Example: `2n+1`\n* `Object`: Provides granular control:\n  * `position`: `number` or An+B string.\n  * `reverse`: `true` to count from the end.\n  * `ofRule`: An ast-grep rule to filter the sibling list before counting.\n\n### range: Position-Based Node Matching\n\nThe `range` rule matches an AST node based on its character-based start and end positions. A `RangeObject` defines `start` and `end` fields, each with 0-based `line` and `column`. `start` is inclusive, `end` is exclusive.\n\n## Relational Rules\n\nRelational rules filter targets based on their position relative to other AST nodes. They can include `stopBy` and `field` options.\n\n### inside: Matching Within a Parent Node\n\nRequires the target node to be inside another node matching the `inside` sub-rule.\n\n```yaml\ninside:\n  pattern: class $C { $$$ }\n  stopBy: end\n```\n\n### has: Matching with a Descendant Node\n\nRequires the target node to have a descendant node matching the `has` sub-rule.\n\n```yaml\nhas:\n  pattern: await $EXPR\n  stopBy: end\n```\n\n### precedes and follows: Sequential Node Matching\n\n* `precedes`: Target node must appear before a node matching the `precedes` sub-rule.\n* `follows`: Target node must appear after a node matching the `follows` sub-rule.\n\nBoth include `stopBy` but not `field`.\n\n### stopBy and field: Refining Relational Searches\n\n**stopBy**: Controls search termination for relational rules.\n\n* `\"neighbor\"` (default): Stops when immediate surrounding node doesn't match.\n* `\"end\"`: Searches to the end of the direction (root for `inside`, leaf for `has`).\n* `Rule object`: Stops when a surrounding node matches the provided rule (inclusive).\n\n**field**: Specifies a sub-node within the target node that should match the relational rule. Only for `inside` and `has`.\n\n**Best Practice**: When unsure, always use `stopBy: end` to ensure the search goes to the end of the direction.\n\n## Composite Rules\n\nComposite rules combine atomic and relational rules using logical operations.\n\n### all: Conjunction (AND) of Rules\n\nMatches a node only if all sub-rules in the list match. Guarantees order of rule matching, important for metavariables.\n\n```yaml\nall:\n  - kind: call_expression\n  - pattern: console.log($ARG)\n```\n\n### any: Disjunction (OR) of Rules\n\nMatches a node if any sub-rules in the list match.\n\n```yaml\nany:\n  - pattern: console.log($ARG)\n  - pattern: console.warn($ARG)\n  - pattern: console.error($ARG)\n```\n\n### not: Negation (NOT) of a Rule\n\nMatches a node if the single sub-rule does not match.\n\n```yaml\nnot:\n  pattern: console.log($ARG)\n```\n\n### matches: Rule Reuse and Utility Rules\n\nTakes a rule-id string, matching if the referenced utility rule matches. Enables rule reuse and recursive rules.\n\n## Metavariables\n\nMetavariables are placeholders in patterns to match dynamic content in the AST.\n\n### $VAR: Single Named Node Capture\n\nCaptures a single named node in the AST.\n\n* **Valid**: `$META`, `$META_VAR`, `$_`\n* **Invalid**: `$invalid`, `$123`, `$KEBAB-CASE`\n* **Example**: `console.log($GREETING)` matches `console.log('Hello World')`.\n* **Reuse**: `$A == $A` matches `a == a` but not `a == b`.\n\n### $$VAR: Single Unnamed Node Capture\n\nCaptures a single unnamed node (e.g., operators, punctuation).\n\n**Example**: To match the operator in `a + b`, use `$$OP`.\n\n```yaml\nrule:\n  kind: binary_expression\n  has:\n    field: operator\n    pattern: $$OP\n```\n\n### $$$MULTI_META_VARIABLE: Multi-Node Capture\n\nMatches zero or more AST nodes (non-greedy). Useful for variable numbers of arguments or statements.\n\n* **Example**: `console.log($$$)` matches `console.log()`, `console.log('hello')`, and `console.log('debug:', key, value)`.\n* **Example**: `function $FUNC($$$ARGS) { $$$ }` matches functions with varying parameters/statements.\n\n### Non-Capturing Metavariables (_VAR)\n\nMetavariables starting with an underscore (`_`) are not captured. They can match different content even if named identically, optimizing performance.\n\n* **Example**: `$_FUNC($_FUNC)` matches `test(a)` and `testFunc(1 + 1)`.\n\n### Important Considerations for Metavariable Detection\n\n* **Syntax Matching**: Only exact metavariable syntax (e.g., `$A`, `$$B`, `$$$C`) is recognized.\n* **Exclusive Content**: Metavariable text must be the only text within an AST node.\n* **Non-working**: `obj.on$EVENT`, `\"Hello $WORLD\"`, `a $OP b`, `$jq`.\n\nThe ast-grep playground is useful for debugging patterns and visualizing metavariables.\n\n## Common Patterns and Examples\n\n### Finding Functions with Specific Content\n\nFind functions that contain await expressions:\n\n```yaml\nrule:\n  kind: function_declaration\n  has:\n    pattern: await $EXPR\n    stopBy: end\n```\n\n### Finding Code Inside Specific Contexts\n\nFind console.log calls inside class methods:\n\n```yaml\nrule:\n  pattern: console.log($$$)\n  inside:\n    kind: method_definition\n    stopBy: end\n```\n\n### Combining Multiple Conditions\n\nFind async functions that use await but don't have try-catch:\n\n```yaml\nrule:\n  all:\n    - kind: function_declaration\n    - has:\n        pattern: await $EXPR\n        stopBy: end\n    - not:\n        has:\n          pattern: try { $$$ } catch ($E) { $$$ }\n          stopBy: end\n```\n\n### Matching Multiple Alternatives\n\nFind any type of console method call:\n\n```yaml\nrule:\n  any:\n    - pattern: console.log($$$)\n    - pattern: console.warn($$$)\n    - pattern: console.error($$$)\n    - pattern: console.debug($$$)\n```\n\n## Troubleshooting Tips\n\n1. **Rule doesn't match**: Use `dump_syntax_tree` to see the actual AST structure\n2. **Relational rule issues**: Ensure `stopBy: end` is set for deep searches\n3. **Wrong node kind**: Check the language's Tree-sitter grammar for correct kind names\n4. **Metavariable not working**: Ensure it's the only content in its AST node\n5. **Pattern too complex**: Break it down into simpler sub-rules using `all`",
  },
  {
    name: "brainstorming",
    description: "Use before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation.",
    template: "# Brainstorming Ideas Into Designs\n\n## Overview\n\nHelp turn ideas into fully formed designs and specs through natural collaborative dialogue.\n\nStart by understanding the current project context, then ask questions one at a time to refine the idea. Once you understand what you're building, present the design in small sections (200-300 words), checking after each section whether it looks right so far.\n\n## The Process\n\n**Understanding the idea:**\n- Check out the current project state first (files, docs, recent commits)\n- Ask questions one at a time to refine the idea\n- Prefer multiple choice questions when possible, but open-ended is fine too\n- Only one question per message - if a topic needs more exploration, break it into multiple questions\n- Focus on understanding: purpose, constraints, success criteria\n\n**Exploring approaches:**\n- Propose 2-3 different approaches with trade-offs\n- Present options conversationally with your recommendation and reasoning\n- Lead with your recommended option and explain why\n\n**Presenting the design:**\n- Once you believe you understand what you're building, present the design\n- Break it into sections of 200-300 words\n- Ask after each section whether it looks right so far\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify if something doesn't make sense\n\n## After the Design\n\n**Documentation:**\n- Write the validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n- Commit the design document to git\n\n**Implementation (if continuing):**\n- Ask: \"Ready to set up for implementation?\"\n- Use `warcraft_skill:writing-plans` to create detailed implementation plan\n\n## Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier to answer than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Be flexible** - Go back and clarify when something doesn't make sense\n- **Challenge assumptions** - Surface fragile assumptions, ask what changes if they fail, offer lean fallback options",
  },
  {
    name: "code-reviewer",
    description: "Use when reviewing implementation changes against an approved plan or task (especially before merging or between Warcraft tasks) to catch missing requirements, YAGNI, dead code, and risky patterns",
    template: "# Code Reviewer\n\n## Overview\n\nThis skill teaches a reviewer to evaluate implementation changes for:\n- Adherence to the approved plan/task (did we build what we said?)\n- Correctness (does it work, including edge cases?)\n- Simplicity (YAGNI, dead code, over-abstraction)\n- Risk (security, performance, maintainability)\n\n**Core principle:** The best change is the smallest correct change that satisfies the plan.\n\n## Iron Laws\n\n- Review against the task/plan first. Code quality comes second.\n- Bias toward deletion and simplification. Every extra line is a liability.\n- Prefer changes that leverage existing patterns and dependencies.\n- Be specific: cite file paths and (when available) line numbers.\n- Do not invent requirements. If the plan/task is ambiguous, mark it and request clarification.\n\n## What Inputs You Need\n\nMinimum:\n- The task intent (1-3 sentences)\n- The plan/task requirements (or a link/path to plan section)\n- The code changes (diff or list of changed files)\n\nIf available (recommended):\n- Acceptance criteria / verification steps from the plan\n- Test output or proof the change was verified\n- Any relevant context files (design decisions, constraints)\n\n## Review Process (In Order)\n\n### 1) Identify Scope\n\n1. List all files changed.\n2. For each file, state why it changed (what requirement it serves).\n3. Flag any changes that do not map to the task/plan.\n\n**Rule:** If you cannot map a change to a requirement, treat it as suspicious until justified.\n\n### 2) Plan/Task Adherence (Non-Negotiable)\n\nCreate a simple checklist:\n- What the task says must happen\n- Evidence in code/tests that it happens\n\nFlag as issues:\n- Missing requirements (implemented behavior does not match intent)\n- Partial implementation with no follow-up task (TODO-driven shipping)\n- Behavior changes that are not in the plan/task\n\n### 3) Correctness Layer\n\nReview for:\n- Edge cases and error paths\n- Incorrect assumptions about inputs/types\n- Inconsistent behavior across platforms/environments\n- Broken invariants (e.g., state can become invalid)\n\nPrefer \"fail fast, fail loud\": invalid states should become clear errors, not silent fallbacks.\n\n### 4) Simplicity / YAGNI Layer\n\nBe ruthless and concrete:\n- Remove dead branches, unused flags/options, unreachable code\n- Remove speculative TODOs and \"reserved for future\" scaffolding\n- Remove comments that restate the code or narrate obvious steps\n- Inline one-off abstractions (helpers/classes/interfaces used once)\n- Replace cleverness with obvious code\n- Reduce nesting with guard clauses / early returns\n\nPrefer clarity over brevity:\n- Avoid nested ternary operators; use `if/else` or `switch` when branches matter\n- Avoid dense one-liners that hide intent or make debugging harder\n\n### 4b) De-Slop Pass (AI Artifacts / Style Drift)\n\nScan the diff (not just the final code) for AI-generated slop introduced in this branch:\n- Extra comments that a human would not add, or that do not match the file's tone\n- Defensive checks or try/catch blocks that are abnormal for that area of the codebase\n  - Especially swallowed errors (\"ignore and continue\") and silent fallbacks\n  - Especially redundant validation in trusted internal codepaths\n- TypeScript escape hatches used to dodge type errors (`as any`, `as unknown as X`) without necessity\n- Style drift: naming, error handling patterns, logging style, and structure inconsistent with nearby code\n\nDefault stance:\n- Prefer deletion over justification.\n- If validation is needed, do it at boundaries; keep internals trusting parsed inputs.\n- If a cast is truly unavoidable, localize it and keep the justification to a single short note.\n\nWhen recommending simplifications, do not accidentally change behavior. If the current behavior is unclear, request clarification or ask for a test that pins it down.\n\n**Default stance:** Do not add extensibility points without an explicit current requirement.\n\n### 5) Risk Layer (Security / Performance / Maintainability)\n\nOnly report what you are confident about.\n\nSecurity checks (examples):\n- No secrets in code/logs\n- No injection vectors (shell/SQL/HTML) introduced\n- Authz/authn checks preserved\n- Sensitive data not leaked\n\nPerformance checks (examples):\n- Avoid unnecessary repeated work (N+1 queries, repeated parsing, repeated filesystem hits)\n- Avoid obvious hot-path allocations or large sync operations\n\nMaintainability checks:\n- Clear naming and intent\n- Consistent error handling\n- API boundaries not blurred\n- Consistent with local file patterns (imports, export style, function style)\n\n### 6) Make One Primary Recommendation\n\nProvide one clear path to reach approval.\nMention alternatives only when they have materially different trade-offs.\n\n### 7) Signal the Investment\n\nTag the required follow-up effort using:\n- Quick (<1h)\n- Short (1-4h)\n- Medium (1-2d)\n- Large (3d+)\n\n## Confidence Filter\n\nOnly report findings you believe are >=80% likely to be correct.\nIf you are unsure, explicitly label it as \"Uncertain\" and explain what evidence would confirm it.\n\n## Output Format (Use This Exactly)\n\n---\n\n**Files Reviewed:** [list]\n\n**Plan/Task Reference:** [task name + link/path to plan section if known]\n\n**Overall Assessment:** [APPROVE | REQUEST_CHANGES | NEEDS_DISCUSSION]\n\n**Bottom Line:** 2-3 sentences describing whether it matches the task/plan and what must change.\n\n### Critical Issues\n- None | [file:line] - [issue] (why it blocks approval) + (recommended fix)\n\n### Major Issues\n- None | [file:line] - [issue] + (recommended fix)\n\n### Minor Issues\n- None | [file:line] - [issue] + (suggested fix)\n\n### YAGNI / Dead Code\n- None | [file:line] - [what to remove/simplify] + (why it is unnecessary)\n\n### Positive Observations\n- [at least one concrete good thing]\n\n### Action Plan\n1. [highest priority change]\n2. [next]\n3. [next]\n\n### Effort Estimate\n[Quick | Short | Medium | Large]\n\n---\n\n## Common Review Smells (Fast Scan)\n\nTask/plan adherence:\n- Adds features not mentioned in the plan/task\n- Leaves TODOs as the mechanism for correctness\n- Introduces new configuration modes/flags \"for future\"\n\nYAGNI / dead code:\n- Options/config that are parsed but not used\n- Branches that do the same thing on both sides\n- Comments like \"reserved for future\" or \"we might need this\"\n\nAI slop / inconsistency:\n- Commentary that restates code, narrates obvious steps, or adds process noise\n- try/catch that swallows errors or returns defaults without a requirement\n- `as any` used to silence type errors instead of fixing types\n- New helpers/abstractions with a single call site\n\nCorrectness:\n- Silent fallbacks to defaults on error when the task expects a hard failure\n- Unhandled error paths, missing cleanup, missing returns\n\nMaintainability:\n- Abstractions used once\n- Unclear naming, \"utility\" grab-bags\n\n## When to Escalate\n\nUse NEEDS_DISCUSSION (instead of REQUEST_CHANGES) when:\n- The plan/task is ambiguous and multiple implementations could be correct\n- The change implies a product/architecture decision not documented\n- Fixing issues requires changing scope, dependencies, or public API",
  },
  {
    name: "dispatching-parallel-agents",
    description: "Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies",
    template: "# Dispatching Parallel Agents\n\n## Overview\n\nWhen you have multiple unrelated failures (different test files, different subsystems, different bugs), investigating them sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Dispatch one agent per independent problem domain. Let them work concurrently.\n\n## Prerequisite: Check Runnable Tasks\n\nBefore dispatching, use `warcraft_status()` to get the **runnable** list — tasks whose dependencies are all satisfied.\n\n**Only dispatch tasks that are runnable.** Never start tasks with unmet dependencies.\n\nOnly `done` satisfies dependencies (not `blocked`, `failed`, `partial`, `cancelled`).\n\n**Ask the operator first:**\n- Use `question()`: \"These tasks are runnable and independent: [list]. Execute in parallel?\"\n- Record the decision with `warcraft_context_write({ name: \"execution-decisions\", content: \"...\" })`\n- Proceed only after operator approval\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Multiple failures?\" [shape=diamond];\n    \"Are they independent?\" [shape=diamond];\n    \"Single agent investigates all\" [shape=box];\n    \"One agent per problem domain\" [shape=box];\n    \"Can they work in parallel?\" [shape=diamond];\n    \"Sequential agents\" [shape=box];\n    \"Parallel dispatch\" [shape=box];\n\n    \"Multiple failures?\" -> \"Are they independent?\" [label=\"yes\"];\n    \"Are they independent?\" -> \"Single agent investigates all\" [label=\"no - related\"];\n    \"Are they independent?\" -> \"Can they work in parallel?\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Parallel dispatch\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Sequential agents\" [label=\"no - shared state\"];\n}\n```\n\n**Use when:**\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n\n**Don't use when:**\n- Failures are related (fix one might fix others)\n- Need to understand full system state\n- Agents would interfere with each other\n\n## The Pattern\n\n### 1. Identify Independent Domains\n\nGroup failures by what's broken:\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\nEach domain is independent - fixing tool approval doesn't affect abort tests.\n\n### 2. Create Focused Agent Tasks\n\nEach agent gets:\n- **Specific scope:** One test file or subsystem\n- **Clear goal:** Make these tests pass\n- **Constraints:** Don't change other code\n- **Expected output:** Summary of what you found and fixed\n\n### 3. Dispatch in Parallel\n\n```typescript\n// Using Warcraft tools for parallel execution\nwarcraft_worktree_create({ task: \"01-fix-abort-tests\" })\nwarcraft_worktree_create({ task: \"02-fix-batch-tests\" })\nwarcraft_worktree_create({ task: \"03-fix-race-condition-tests\" })\n// All three run concurrently in isolated worktrees\n```\n\nParallelize by issuing multiple task() calls in the same assistant message.\n\n```typescript\ntask({ subagent_type: 'brann', prompt: 'Investigate failure A' })\ntask({ subagent_type: 'brann', prompt: 'Investigate failure B' })\n```\n\n### 4. Review and Integrate\n\nWhen agents return:\n- Read each summary\n- Verify fixes don't conflict\n- Run full test suite\n- Integrate all changes with `warcraft_merge`\n\n## Agent Prompt Structure\n\nGood agent prompts are:\n1. **Focused** - One clear problem domain\n2. **Self-contained** - All context needed to understand the problem\n3. **Specific about output** - What should the agent return?\n\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nDo NOT just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n## Common Mistakes\n\n**❌ Too broad:** \"Fix all the tests\" - agent gets lost\n**✅ Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n**❌ No context:** \"Fix the race condition\" - agent doesn't know where\n**✅ Context:** Paste the error messages and test names\n\n**❌ No constraints:** Agent might refactor everything\n**✅ Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n**❌ Vague output:** \"Fix it\" - you don't know what changed\n**✅ Specific:** \"Return summary of root cause and changes\"\n\n## When NOT to Use\n\n**Related failures:** Fixing one might fix others - investigate together first\n**Need full context:** Understanding requires seeing entire system\n**Exploratory debugging:** You don't know what's broken yet\n**Shared state:** Agents would interfere (editing same files, using same resources)\n\n## Real Example from Session\n\n**Scenario:** 6 test failures across 3 files after major refactoring\n\n**Failures:**\n- agent-tool-abort.test.ts: 3 failures (timing issues)\n- batch-completion-behavior.test.ts: 2 failures (tools not executing)\n- tool-approval-race-conditions.test.ts: 1 failure (execution count = 0)\n\n**Decision:** Independent domains - abort logic separate from batch completion separate from race conditions\n\n**Dispatch:**\n```\nAgent 1 → Fix agent-tool-abort.test.ts\nAgent 2 → Fix batch-completion-behavior.test.ts\nAgent 3 → Fix tool-approval-race-conditions.test.ts\n```\n\n**Results:**\n- Agent 1: Replaced timeouts with event-based waiting\n- Agent 2: Fixed event structure bug (threadId in wrong place)\n- Agent 3: Added wait for async tool execution to complete\n\n**Integration:** All fixes independent, no conflicts, full suite green\n\n**Time saved:** 3 problems solved in parallel vs sequentially\n\n## Key Benefits\n\n1. **Parallelization** - Multiple investigations happen simultaneously\n2. **Focus** - Each agent has narrow scope, less context to track\n3. **Independence** - Agents don't interfere with each other\n4. **Speed** - 3 problems solved in time of 1\n\n## Verification\n\nAfter agents return:\n1. **Review each summary** - Understand what changed\n2. **Check for conflicts** - Did agents edit same code?\n3. **Run full suite** - Verify all fixes work together\n4. **Spot check** - Agents can make systematic errors\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes",
  },
  {
    name: "docker-mastery",
    description: "Use when working with Docker containers — debugging container failures, writing Dockerfiles, docker-compose for integration tests, image optimization, or deploying containerized applications",
    template: "# Docker Mastery\n\n## Overview\n\nDocker is a **platform for building, shipping, and running applications**, not just isolation.\n\nAgents should think in containers: reproducible environments, declarative dependencies, isolated execution.\n\n**Core principle:** Containers are not virtual machines. They share the kernel but isolate processes, filesystems, and networks.\n\n**Violating the letter of these guidelines is violating the spirit of containerization.**\n\n## The Iron Law\n\n```\nUNDERSTAND THE CONTAINER BEFORE DEBUGGING INSIDE IT\n```\n\nBefore exec'ing into a container or adding debug commands:\n1. Check the image (what's installed?)\n2. Check mounts (what host files are visible?)\n3. Check environment variables (what config is passed?)\n4. Check the Dockerfile (how was it built?)\n\nRandom debugging inside containers wastes time. Context first, then debug.\n\n## When to Use\n\nUse this skill when working with:\n- **Container build failures** - Dockerfile errors, missing dependencies\n- **Test environment setup** - Reproducible test environments across machines\n- **Integration test orchestration** - Multi-service setups (DB + API + tests)\n- **Dockerfile authoring** - Writing efficient, maintainable Dockerfiles\n- **Image size optimization** - Reducing image size, layer caching\n- **Deployment** - Containerized application deployment\n- **Sandbox debugging** - Issues with Warcraft's Docker sandbox mode\n\n**Use this ESPECIALLY when:**\n- Tests pass locally but fail in CI (environment mismatch)\n- \"Works on my machine\" problems\n- Need to test against specific dependency versions\n- Multiple services must coordinate (database + API)\n- Building for production deployment\n\n## Core Concepts\n\n### Images vs Containers\n\n- **Image**: Read-only template (built from Dockerfile)\n- **Container**: Running instance of an image (ephemeral by default)\n\n```bash\n# Build once\ndocker build -t myapp:latest .\n\n# Run many times\ndocker run --rm myapp:latest\ndocker run --rm -e DEBUG=true myapp:latest\n```\n\n**Key insight:** Changes inside containers are lost unless committed or volumes are used.\n\n### Volumes & Mounts\n\nMount host directories into containers for persistence and code sharing:\n\n```bash\n# Mount current directory to /app in container\ndocker run -v $(pwd):/app myapp:latest\n\n# Warcraft worktrees are mounted automatically\n# Your code edits (via Read/Write/Edit tools) affect the host\n# Container sees the same files at runtime\n```\n\n**How Warcraft uses this:** Worktree is mounted into container, so file tools work on host, bash commands run in container.\n\n### Multi-Stage Builds\n\nMinimize image size by using multiple FROM statements:\n\n```dockerfile\n# Build stage (large, has compilers)\nFROM node:22 AS builder\nWORKDIR /app\nCOPY package.json bun.lockb ./\nRUN bun install\nCOPY . .\nRUN bun run build\n\n# Runtime stage (small, production only)\nFROM node:22-slim\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCMD [\"node\", \"dist/index.js\"]\n```\n\n**Result:** Builder tools (TypeScript, bundlers) not included in final image.\n\n### Docker Compose for Multi-Service Setups\n\nDefine multiple services in `docker-compose.yml`:\n\n```yaml\nversion: '3.8'\nservices:\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: testpass\n    ports:\n      - \"5432:5432\"\n  \n  api:\n    build: .\n    environment:\n      DATABASE_URL: postgres://db:5432/testdb\n    depends_on:\n      - db\n    ports:\n      - \"3000:3000\"\n```\n\nRun with: `docker-compose up -d`\nTeardown with: `docker-compose down`\n\n### Network Modes\n\n- **bridge** (default): Isolated network, containers can talk to each other by name\n- **host**: Container uses host's network directly (no isolation)\n- **none**: No network access\n\n**When to use host mode:** Debugging network issues, accessing host services directly.\n\n## Common Patterns\n\n### Debug a Failing Container\n\n**Problem:** Container exits immediately, logs unclear.\n\n**Pattern:**\n1. Run interactively with shell:\n   ```bash\n   docker run -it --entrypoint sh myapp:latest\n   ```\n2. Inspect filesystem, check if dependencies exist:\n   ```bash\n   ls /app\n   which node\n   cat /etc/os-release\n   ```\n3. Run command manually to see full error:\n   ```bash\n   node dist/index.js\n   ```\n\n### Integration Tests with Docker Compose\n\n**Pattern:**\n1. Define services in `docker-compose.test.yml`\n2. Add wait logic (wait for DB to be ready)\n3. Run tests\n4. Teardown\n\n```yaml\n# docker-compose.test.yml\nservices:\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: test\n  test:\n    build: .\n    command: bun run test:integration\n    depends_on:\n      - db\n    environment:\n      DATABASE_URL: postgres://postgres:test@db:5432/testdb\n```\n\n```bash\ndocker-compose -f docker-compose.test.yml up --abort-on-container-exit\ndocker-compose -f docker-compose.test.yml down\n```\n\n### Optimize Dockerfile\n\n**Anti-pattern:**\n```dockerfile\nFROM node:22\nWORKDIR /app\nCOPY . .              # Copies everything (including node_modules, .git)\nRUN bun install       # Invalidates cache on any file change\nCMD [\"bun\", \"run\", \"start\"]\n```\n\n**Optimized:**\n```dockerfile\nFROM node:22-slim     # Use slim variant\nWORKDIR /app\n\n# Copy dependency files first (cache layer)\nCOPY package.json bun.lockb ./\nRUN bun install --production\n\n# Copy source code (changes frequently)\nCOPY src ./src\nCOPY tsconfig.json ./\n\nCMD [\"bun\", \"run\", \"start\"]\n```\n\n**Add `.dockerignore`:**\n```\nnode_modules\n.git\n.env\n*.log\ndist\n.DS_Store\n```\n\n### Handle Missing Dependencies\n\n**Problem:** Command fails with \"not found\" in container.\n\n**Pattern:**\n1. Check if dependency is in image:\n   ```bash\n   docker run -it myapp:latest which git\n   ```\n2. If missing, add to Dockerfile:\n   ```dockerfile\n   RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n   ```\n3. Or use a richer base image (e.g., `node:22` instead of `node:22-slim`).\n\n## Warcraft Sandbox Integration\n\n### How Warcraft Wraps Commands\n\nWhen sandbox mode is active (`sandbox: 'docker'` in config):\n1. Warcraft hook intercepts bash commands before execution\n2. Wraps with `docker run --rm -v <worktree>:/workspace -w /workspace <image> sh -c \"<command>\"`\n3. Command runs in container, but file edits (Read/Write/Edit) still affect host\n\n**Workers are unaware** — they issue normal bash commands, Warcraft handles containerization.\n\n### When Host Access is Needed\n\nSome operations MUST run on host:\n- **Git operations** (commit, push, branch) — repo state is on host\n- **Host-level tools** (Docker itself, system config)\n- **Cross-worktree operations** (accessing main repo from worktree)\n\n**Pattern:** Use `HOST:` prefix to escape sandbox:\n```bash\nHOST: git status\nHOST: docker ps\n```\n\n**If you need host access frequently:** Report as blocked and ask user if sandbox should be disabled for this task.\n\n### Persistent vs Ephemeral Containers\n\n**Current (v1.2.0):** Each command runs `docker run --rm` (ephemeral). State does NOT persist.\n\nExample: `npm install lodash` in one command → not available in next command.\n\n**Workaround:** Install dependencies in Dockerfile, not at runtime.\n\n**Future:** `docker exec` will reuse containers, persisting state across commands.\n\n### Auto-Detected Images\n\nWarcraft detects runtime from project files:\n- `package.json` → `node:22-slim`\n- `requirements.txt` / `pyproject.toml` → `python:3.12-slim`\n- `go.mod` → `golang:1.22-slim`\n- `Cargo.toml` → `rust:1.77-slim`\n- `Dockerfile` → Builds from project Dockerfile\n- Fallback → `ubuntu:24.04`\n\n**Override:** Set `dockerImage` in config (`~/.config/opencode/opencode_warcraft.json`).\n\n## Red Flags - STOP\n\nIf you catch yourself:\n- Installing packages on host instead of in Dockerfile\n- Running `docker build` without `.dockerignore` (cache invalidation)\n- Using `latest` tag in production (non-reproducible)\n- Ignoring container exit codes (hides failures)\n- Assuming state persists between `docker run --rm` commands\n- Using absolute host paths in Dockerfile (not portable)\n- Copying secrets into image layers (leaks credentials)\n\n**ALL of these mean: STOP. Review pattern.**\n\n## Anti-Patterns\n\n| Excuse | Reality |\n|--------|---------|\n| \"I'll just run it on host\" | Container mismatch bugs are worse to debug later. Build happens in container anyway. |\n| \"Works in my container, don't need CI\" | CI uses different cache state. Always test in CI-like environment. |\n| \"I'll optimize the Dockerfile later\" | Later never comes. Large images slow down deployments now. |\n| \"latest tag is fine for dev\" | Dev should match prod. Pin versions or face surprises. |\n| \"Don't need .dockerignore, COPY is fast\" | Invalidates cache on every file change. Wastes minutes per build. |\n| \"Install at runtime, not in image\" | Ephemeral containers lose state. Slows down every command. |\n| \"Skip depends_on, services start fast\" | Race conditions in integration tests. Use wait-for-it or health checks. |\n\n## Verification Before Completion\n\nBefore marking Docker work complete:\n\n- [ ] Container runs successfully: `docker run --rm <image> <command>` exits 0\n- [ ] Tests pass inside container (not just on host)\n- [ ] No host pollution (dependencies installed in container, not host)\n- [ ] `.dockerignore` exists if using `COPY . .`\n- [ ] Image tags are pinned (not `latest`) for production\n- [ ] Multi-stage build used if applicable (separate build/runtime)\n- [ ] Integration tests teardown properly (`docker-compose down`)\n\n**If any fail:** Don't claim success. Fix or report blocker.\n\n## Quick Reference\n\n| Task | Command Pattern |\n|------|----------------|\n| **Debug container** | `docker run -it --entrypoint sh <image>` |\n| **Run with mounts** | `docker run -v $(pwd):/app <image>` |\n| **Multi-service tests** | `docker-compose up --abort-on-container-exit` |\n| **Check image contents** | `docker run --rm <image> ls /app` |\n| **Optimize build** | Add `.dockerignore`, use multi-stage, pin versions |\n| **Escape Warcraft sandbox** | Prefix with `HOST:` (e.g., `HOST: git status`) |\n\n## Related Skills\n\n- **warcraft_skill:systematic-debugging** - When container behavior is unexpected\n- **warcraft_skill:test-driven-development** - Write tests that run in containers\n- **warcraft_skill:verification-before-completion** - Verify tests pass in container before claiming done",
  },
  {
    name: "executing-plans",
    description: "Use when you have a written implementation plan to execute in a separate session with review checkpoints",
    template: "# Executing Plans\n\n## Overview\n\nLoad plan, review critically, execute tasks in batches, report for review between batches.\n\n**Core principle:** Batch execution with checkpoints for architect review.\n\n**Announce at start:** \"I'm using the executing-plans skill to implement this plan.\"\n\n## The Process\n\n### Step 1: Load and Review Plan\n1. Read plan file\n2. Review critically - identify any questions or concerns about the plan\n3. If concerns: Raise them with your human partner before starting\n4. If no concerns: Create TodoWrite and proceed\n\n### Step 2: Identify Runnable Tasks\n\nUse `warcraft_status()` to get the **runnable** list — tasks with all dependencies satisfied.\n\nOnly `done` satisfies dependencies (not `blocked`, `failed`, `partial`, `cancelled`).\n\n**When 2+ tasks are runnable:**\n- **Ask the operator** via `question()`: \"Multiple tasks are runnable: [list]. Run in parallel, sequential, or a specific subset?\"\n- Record the decision with `warcraft_context_write({ name: \"execution-decisions\", content: \"...\" })` for future reference\n\n**When 1 task is runnable:** Proceed directly.\n\n### Step 3: Execute Batch\n\nFor each task in the batch:\n1. Mark as in_progress via `warcraft_worktree_create()`\n2. Follow each step exactly (plan has bite-sized steps)\n3. Run verifications as specified\n4. Mark as completed\n\n### Step 4: Report\nWhen batch complete:\n- Show what was implemented\n- Show verification output\n- Say: \"Ready for feedback.\"\n\n### Step 4.5: Post-Batch Algalon Review\n\nAfter the batch report, ask the operator if they want an Algalon code review for the batch.\nIf yes, run `task({ subagent_type: \"algalon\", prompt: \"Review implementation changes from the latest batch.\" })` and apply feedback before starting the next batch.\n\n### Step 5: Continue\nBased on feedback:\n- Apply changes if needed\n- Execute next batch\n- Repeat until complete\n\n### Step 6: Complete Development\n\nAfter all tasks complete and verified:\n- Announce: \"I'm using the verification-before-completion skill to complete this work.\"\n- **REQUIRED SUB-SKILL:** Use warcraft_skill:verification-before-completion\n- Follow that skill to verify tests, present options, execute choice\n\n## When to Stop and Ask for Help\n\n**STOP executing immediately when:**\n- Hit a blocker mid-batch (missing dependency, test fails, instruction unclear)\n- Plan has critical gaps preventing starting\n- You don't understand an instruction\n- Verification fails repeatedly\n\n**Ask for clarification rather than guessing.**\n\n## When to Revisit Earlier Steps\n\n**Return to Review (Step 1) when:**\n- Partner updates the plan based on your feedback\n- Fundamental approach needs rethinking\n\n**Don't force through blockers** - stop and ask.\n\n## Remember\n- Review plan critically first\n- Follow plan steps exactly\n- Don't skip verifications\n- Reference skills when plan says to\n- Between batches: just report and wait\n- Stop when blocked, don't guess",
  },
  {
    name: "finishing-a-development-branch",
    description: "Use when implementation is complete and you need to choose how to integrate, preserve, or discard worktree changes safely.",
    template: "# Finishing a Development Branch\n\n## Overview\n\nClose a task deliberately: verify evidence, choose an integration path, then clean up.\n\nIn this project, prefer Warcraft tools over raw git plumbing.\n\n## Preconditions\n\nBefore presenting options:\n- Task implementation is done in its worktree\n- Verification evidence exists (tests/checks run for the touched scope)\n- `warcraft_worktree_commit` has been run if code should be preserved\n\nIf verification is missing, stop and gather it first.\n\n## Completion Options\n\nPresent exactly these options:\n\n1. Merge task branch now (`warcraft_merge`)\n2. Keep branch/worktree for later\n3. Discard branch/worktree (`warcraft_worktree_discard`)\n4. Export branch for PR workflow, merge later\n\n## Execution Guidance\n\n### Option 1 — Merge now\n- Run `warcraft_merge` for the task\n- Confirm merge result and current branch state\n\n### Option 2 — Keep as-is\n- Leave worktree and branch intact\n- Report location/status so it can be resumed later\n\n### Option 3 — Discard\n- Require explicit confirmation before destructive action\n- Run `warcraft_worktree_discard`\n\n### Option 4 — PR workflow\n- Keep branch/worktree intact\n- Provide evidence summary (what changed + verification commands)\n- Merge later after PR review\n\n## Red Flags\n\nNever:\n- Merge without verification evidence\n- Discard without explicit confirmation\n- Hide unresolved failures at handoff\n\nAlways:\n- State which option was chosen\n- State what was merged/kept/discarded\n- State any follow-up required",
  },
  {
    name: "parallel-exploration",
    description: "Use when you need parallel, read-only exploration with task() (Brann fan-out)",
    template: "# Parallel Exploration (Brann Fan-Out)\n\n## Overview\n\nWhen you need to answer \"where/how does X work?\" across multiple domains (codebase, tests, docs, OSS), investigating sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Decompose into independent sub-questions, spawn one task per sub-question, collect results asynchronously.\n\n**Safe in Planning mode:** This is read-only exploration. It is OK to use during exploratory research even when there is no feature, no plan, and no approved tasks.\n\n**This skill is for read-only research.** For parallel implementation work, use `warcraft_skill(\"dispatching-parallel-agents\")` with `warcraft_worktree_create`.\n\n## When to Use\n\n**Default to this skill when:**\n**Use when:**\n- Investigation spans multiple domains (code + tests + docs)\n- User asks **2+ questions across different domains** (e.g., code + tests, code + docs/OSS, code + config/runtime)\n- Questions are independent (answer to A doesn't affect B)\n- User asks **3+ independent questions** (often as a numbered list or separate bullets)\n- No edits needed (read-only exploration)\n- User asks for an explorationthat likely spans multiple files/packages\n- The work is read-only and the questions can be investigated independently\n\n**Only skip this skill when:**\n- Investigation requires shared state or context between questions\n- It's a single focused question that is genuinely answerable with **one quick grep + one file read**\n- Questions are dependent (answer A materially changes what to ask for B)\n- Work involves file edits (use Warcraft tasks / Forager instead)\n\n**Important:** Do not treat \"this is exploratory\" as a reason to avoid delegation. This skill is specifically for exploratory research when fan-out makes it faster and cleaner.\n\n## The Pattern\n\n### 1. Decompose Into Independent Questions\n\nSplit your investigation into 2-4 independent sub-questions. Good decomposition:\n\n| Domain | Question Example |\n|--------|------------------|\n| Codebase | \"Where is X implemented? What files define it?\" |\n| Tests | \"How is X tested? What test patterns exist?\" |\n| Docs/OSS | \"How do other projects implement X? What's the recommended pattern?\" |\n| Config | \"How is X configured? What environment variables affect it?\" |\n\n**Bad decomposition (dependent questions):**\n- \"What is X?\" then \"How is X used?\" (second depends on first)\n- \"Find the bug\" then \"Fix the bug\" (not read-only)\n\n### 2. Spawn Tasks (Fan-Out)\n\nLaunch all tasks before waiting for any results:\n\n```typescript\n// Parallelize by issuing multiple task() calls in the same assistant message.\ntask({\n  subagent_type: 'brann',\n  description: 'Find API route implementation',\n  prompt: `Where are API routes implemented and registered?\n    - Find the tool definition\n    - Find the plugin registration\n    - Return file paths with line numbers`,\n});\n\ntask({\n  subagent_type: 'brann',\n  description: 'Analyze background task concurrency',\n  prompt: `How does background task concurrency/queueing work?\n    - Find the manager/scheduler code\n    - Document the concurrency model\n    - Return file paths with evidence`,\n});\n\ntask({\n  subagent_type: 'brann',\n  description: 'Find parent notification mechanism',\n  prompt: `How does parent notification work for background tasks?\n    - Where is the notification built?\n    - How is it sent to the parent session?\n    - Return file paths with evidence`,\n});\n```\n\n**Key points:**\n- Use `subagent_type: 'brann'` for read-only exploration\n- Give each task a clear, focused `description`\n- Make prompts specific about what evidence to return\n\n### 3. Continue Working (Optional)\n\nWhile tasks run, you can:\n- Work on other aspects of the problem\n- Prepare synthesis structure\n- Start drafting based on what you already know\n\nYou'll receive a `<system-reminder>` notification when each task completes.\n\n### 4. Collect Results\n\nWhen each task completes, its result is returned directly. Collect the outputs from each task and proceed to synthesis.\n\n### 5. Synthesize Findings\n\nCombine results from all tasks:\n- Cross-reference findings (file X mentioned by tasks A and B)\n- Identify gaps (task C found nothing, need different approach)\n- Build coherent answer from parallel evidence\n\n### 6. Cleanup (If Needed)\n\nNo manual cancellation is required in task mode.\n\n## Prompt Templates\n\n### Codebase Slice\n\n```\nInvestigate [TOPIC] in the codebase:\n- Where is [X] defined/implemented?\n- What files contain [X]?\n- How does [X] interact with [Y]?\n\nReturn:\n- File paths with line numbers\n- Brief code snippets as evidence\n- Key patterns observed\n```\n\n### Tests Slice\n\n```\nInvestigate how [TOPIC] is tested:\n- What test files cover [X]?\n- What testing patterns are used?\n- What edge cases are tested?\n\nReturn:\n- Test file paths\n- Example test patterns\n- Coverage gaps if obvious\n```\n\n### Docs/OSS Slice\n\n```\nResearch [TOPIC] in external sources:\n- How do other projects implement [X]?\n- What does the official documentation say?\n- What are common patterns/anti-patterns?\n\nReturn:\n- Links to relevant docs/repos\n- Key recommendations\n- Patterns that apply to our codebase\n```\n\n## Real Example\n\n**Investigation:** \"How does the API routing system work?\"\n\n**Decomposition:**\n1. Implementation: Where are API routes defined?\n2. Routing: How does route registration work?\n3. Notifications: How are errors surfaced to the caller?\n\n**Fan-out:**\n```typescript\n// Parallelize by issuing multiple task() calls in the same assistant message.\ntask({\n  subagent_type: 'brann',\n  description: 'Find API route implementation',\n  prompt: 'Where are API routes implemented? Find tool definition and registration.',\n});\n\ntask({\n  subagent_type: 'brann',\n  description: 'Analyze concurrency model',\n  prompt: 'How does background task concurrency work? Find the manager/scheduler.',\n});\n\ntask({\n  subagent_type: 'brann',\n  description: 'Find notification mechanism',\n  prompt: 'How are parent sessions notified of task completion?',\n});\n```\n\n**Results:**\n- Task 1: Found `background-tools.ts` (tool definition), `index.ts` (registration)\n- Task 2: Found `manager.ts` with concurrency=3 default, queue-based scheduling\n- Task 3: Found `session.prompt()` call in manager for parent notification\n\n**Synthesis:** Complete picture of background task lifecycle in ~1/3 the time of sequential investigation.\n\n## Common Mistakes\n\n**Spawning sequentially (defeats the purpose):**\n```typescript\n// BAD: Wait for each before spawning next\nawait task({ ... });\nawait task({ ... });\n```\n\n```typescript\n// GOOD: Spawn all in the same assistant message\ntask({ ... });\ntask({ ... });\ntask({ ... });\n```\n\n**Too many tasks (diminishing returns):**\n- 2-4 tasks: Good parallelization\n- 5+ tasks: Overhead exceeds benefit, harder to synthesize\n\n**Dependent questions:**\n- Don't spawn task B if it needs task A's answer\n- Either make them independent or run sequentially\n\n**Using for edits:**\n- Scout is read-only; use Forager for implementation\n- This skill is for exploration, not execution\n\n## Key Benefits\n\n1. **Speed** - 3 investigations in time of 1\n2. **Focus** - Each Scout has narrow scope\n3. **Independence** - No interference between tasks\n4. **Flexibility** - Cancel unneeded tasks, add new ones\n\n## Verification\n\nAfter using this pattern, verify:\n- [ ] All tasks spawned before collecting any results (true fan-out)\n- [ ] Verified `task()` fan-out pattern used for parallel exploration\n- [ ] Synthesized findings into coherent answer",
  },
  {
    name: "receiving-code-review",
    description: "Use when review feedback arrives and you must evaluate, clarify, and apply it with technical rigor.",
    template: "# Receiving Code Review\n\n## Overview\n\nTreat review as technical input, not ceremony.\n\nCore loop:\n1. Understand each comment\n2. Verify against code and requirements\n3. Apply or push back with evidence\n4. Re-test\n\n## Response Rules\n\nFor every review item:\n- Restate the technical claim\n- Confirm whether it is valid in this codebase\n- Make the smallest correct change\n- Show verification evidence\n\nIf unclear:\n- Pause implementation\n- Ask precise clarification questions\n\nIf you disagree:\n- Explain with concrete evidence (code path, tests, constraints)\n- Propose a safer alternative\n\n## Priority Order\n\n1. Correctness and safety defects\n2. Requirement mismatches\n3. Maintainability and simplification\n4. Cosmetic/style suggestions\n\n## Red Flags\n\nNever:\n- Blindly implement unverified suggestions\n- Ignore critical findings because tests are currently green\n- Continue when requirements are still ambiguous\n\nAlways:\n- Resolve ambiguity first\n- Verify each meaningful fix\n- Keep a short audit trail of what changed and why",
  },
  {
    name: "requesting-code-review",
    description: "Use when finishing a task or milestone and you need an explicit review pass before merge or handoff.",
    template: "# Requesting Code Review\n\n## Overview\n\nAsk for review before merge so defects are caught while context is fresh.\n\n## When to Trigger\n\nMandatory:\n- After significant task implementation\n- Before `warcraft_merge`\n- When changing workflow-critical behavior\n\nOptional but useful:\n- After complex refactors\n- When uncertain about edge cases or scope drift\n\n## Review Request Pattern\n\n1. Summarize intent and constraints\n   - Task/plan goal\n   - Files changed\n   - Verification already run\n\n2. Dispatch reviewer\n   - Use an Algalon/reviewer subagent for critical review\n   - Ask for: correctness, requirement coverage, risk, and YAGNI\n\n3. Apply feedback by severity\n   - Critical/high: fix before merge\n   - Medium/low: fix or document why deferred\n\n4. Re-verify after fixes\n\n## Minimal Review Prompt Template\n\n- Requirement: <what had to be built>\n- Scope: <changed files>\n- Verification: <commands + result>\n- Ask: Identify correctness gaps, missing requirements, risky patterns, and unnecessary complexity.\n\n## Red Flags\n\nNever:\n- Skip review because a change “looks small”\n- Merge with unresolved critical feedback\n\nAlways:\n- Review against requirements first\n- Include concrete evidence in the request\n- Re-run relevant verification after applying fixes",
  },
  {
    name: "subagent-driven-development",
    description: "Use when executing an approved multi-task plan in the current session with fresh worker context per task.",
    template: "# Subagent-Driven Development\n\n## Overview\n\nExecute one approved task at a time with isolated worktrees and review gates.\n\nCore principle: fresh worker per task + explicit review before merge.\n\n## Preconditions\n\n- Plan is approved\n- Tasks are synced\n- Runnable tasks are known (`warcraft_status`)\n\n## Per-Task Loop\n\n1. Select next runnable task\n2. Start execution with `warcraft_worktree_create`\n3. Delegate implementation to worker subagent\n4. Require task-level verification evidence\n5. Run review pass (Algalon/reviewer)\n6. Apply fixes and re-verify\n7. Finalize via `warcraft_worktree_commit`\n8. Merge (or keep/discard) using finishing workflow\n\n## Dispatch Guidance\n\nEach worker assignment should include:\n- Exact task objective\n- File scope constraints\n- Acceptance criteria\n- Required verification commands\n\n## Quality Gates\n\nBefore marking task done:\n- Requirements satisfied\n- No unresolved critical findings\n- Verification command output captured\n- Task report updated\n\n## Red Flags\n\nNever:\n- Run multiple tasks in parallel when dependencies are unresolved\n- Skip review because implementation “seems fine”\n- Merge without passing verification evidence",
  },
  {
    name: "systematic-debugging",
    description: "Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes",
    template: "# Systematic Debugging\n\n## Overview\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n**Violating the letter of this process is violating the spirit of debugging.**\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Use this ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n- You don't fully understand the issue\n\n**Don't skip when:**\n- Issue seems simple (simple bugs have root causes too)\n- You're in a hurry (rushing guarantees rework)\n- Manager wants it fixed NOW (systematic is faster than thrashing)\n\n## The Four Phases\n\nYou MUST complete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - They often contain the exact solution\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - Does it happen every time?\n   - If not reproducible → gather more data, don't guess\n\n3. **Check Recent Changes**\n   - What changed that could cause this?\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n\n   **WHEN system has multiple components (CI → build → signing, API → service → database):**\n\n   **BEFORE proposing fixes, add diagnostic instrumentation:**\n   ```\n   For EACH component boundary:\n     - Log what data enters component\n     - Log what data exits component\n     - Verify environment/config propagation\n     - Check state at each layer\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN analyze evidence to identify failing component\n   THEN investigate that specific component\n   ```\n\n   **Example (multi-layer system):**\n   ```bash\n   # Layer 1: Workflow\n   echo \"=== Secrets available in workflow: ===\"\n   echo \"IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}\"\n\n   # Layer 2: Build script\n   echo \"=== Env vars in build script: ===\"\n   env | grep IDENTITY || echo \"IDENTITY not in environment\"\n\n   # Layer 3: Signing script\n   echo \"=== Keychain state: ===\"\n   security list-keychains\n   security find-identity -v\n\n   # Layer 4: Actual signing\n   codesign --sign \"$IDENTITY\" --verbose=4 \"$APP\"\n   ```\n\n   **This reveals:** Which layer fails (secrets → workflow ✓, workflow → build ✗)\n\n5. **Trace Data Flow**\n\n   **WHEN error is deep in call stack:**\n\n   See `root-cause-tracing.md` in this directory for the complete backward tracing technique.\n\n   **Quick version:**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n**Find the pattern before fixing:**\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - If implementing pattern, read reference implementation COMPLETELY\n   - Don't skim - read every line\n   - Understand the pattern fully before applying\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n   - Don't assume \"that can't matter\"\n\n4. **Understand Dependencies**\n   - What other components does this need?\n   - What settings, config, environment?\n   - What assumptions does it make?\n\n### Phase 3: Hypothesis and Testing\n\n**Scientific method:**\n\n1. **Form Single Hypothesis**\n   - State clearly: \"I think X is the root cause because Y\"\n   - Write it down\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change to test hypothesis\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes → Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n4. **When You Don't Know**\n   - Say \"I don't understand X\"\n   - Don't pretend to know\n   - Ask for help\n   - Research more\n\n### Phase 4: Implementation\n\n**Fix the root cause, not the symptom:**\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - Automated test if possible\n   - One-off test script if no framework\n   - MUST have before fixing\n   - Use the `warcraft_skill:test-driven-development` skill for writing proper failing tests\n\n2. **Implement Single Fix**\n   - Address the root cause identified\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n   - No bundled refactoring\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n   - Issue actually resolved?\n\n4. **If Fix Doesn't Work**\n   - STOP\n   - Count: How many fixes have you tried?\n   - If < 3: Return to Phase 1, re-analyze with new information\n   - **If ≥ 3: STOP and question the architecture (step 5 below)**\n   - DON'T attempt Fix #4 without architectural discussion\n\n5. **If 3+ Fixes Failed: Question Architecture**\n\n   **Pattern indicating architectural problem:**\n   - Each fix reveals new shared state/coupling/problem in different place\n   - Fixes require \"massive refactoring\" to implement\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals:**\n   - Is this pattern fundamentally sound?\n   - Are we \"sticking with it through sheer inertia\"?\n   - Should we refactor architecture vs. continue fixing symptoms?\n\n   **Discuss with your human partner before attempting more fixes**\n\n   This is NOT a failed hypothesis - this is a wrong architecture.\n\n## Red Flags - STOP and Follow Process\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"Add multiple changes, run tests\"\n- \"Skip the test, I'll manually verify\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Pattern says X but I'll adapt it differently\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- **\"One more fix attempt\" (when already tried 2+)**\n- **Each fix reveals new problem in different place**\n\n**ALL of these mean: STOP. Return to Phase 1.**\n\n**If 3+ fixes failed:** Question the architecture (see Phase 4.5)\n\n## your human partner's Signals You're Doing It Wrong\n\n**Watch for these redirections:**\n- \"Is that not happening?\" - You assumed without verifying\n- \"Will it show us...?\" - You should have added evidence gathering\n- \"Stop guessing\" - You're proposing fixes without understanding\n- \"Ultrathink this\" - Question fundamentals, not just symptoms\n- \"We're stuck?\" (frustrated) - Your approach isn't working\n\n**When you see these:** STOP. Return to Phase 1.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple, don't need process\" | Simple issues have root causes too. Process is fast for simple bugs. |\n| \"Emergency, no time for process\" | Systematic debugging is FASTER than guess-and-check thrashing. |\n| \"Just try this first, then investigate\" | First fix sets the pattern. Do it right from the start. |\n| \"I'll write test after confirming fix works\" | Untested fixes don't stick. Test first proves it. |\n| \"Multiple fixes at once saves time\" | Can't isolate what worked. Causes new bugs. |\n| \"Reference too long, I'll adapt the pattern\" | Partial understanding guarantees bugs. Read it completely. |\n| \"I see the problem, let me fix it\" | Seeing symptoms ≠ understanding root cause. |\n| \"One more fix attempt\" (after 2+ failures) | 3+ failures = architectural problem. Question pattern, don't fix again. |\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n## When Process Reveals \"No Root Cause\"\n\nIf systematic investigation reveals issue is truly environmental, timing-dependent, or external:\n\n1. You've completed the process\n2. Document what you investigated\n3. Implement appropriate handling (retry, timeout, error message)\n4. Add monitoring/logging for future investigation\n\n**But:** 95% of \"no root cause\" cases are incomplete investigation.\n\n## Supporting Techniques\n\nThese techniques are part of systematic debugging and available in this directory:\n\n- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger\n- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause\n- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling\n\n**Related skills:**\n- **warcraft_skill:test-driven-development** - For creating failing test case (Phase 4, Step 1)\n- **warcraft_skill:verification-before-completion** - Verify fix worked before claiming success\n\n## Real-World Impact\n\nFrom debugging sessions:\n- Systematic approach: 15-30 minutes to fix\n- Random fixes approach: 2-3 hours of thrashing\n- First-time fix rate: 95% vs 40%\n- New bugs introduced: Near zero vs common",
  },
  {
    name: "test-driven-development",
    description: "Use when implementing any feature or bugfix, before writing implementation code",
    template: "# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```dot\ndigraph tdd_cycle {\n    rankdir=LR;\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\n    next [label=\"Next\", shape=ellipse];\n\n    red -> verify_red;\n    verify_red -> green [label=\"yes\"];\n    verify_red -> red [label=\"wrong\\nfailure\"];\n    green -> verify_green;\n    verify_green -> refactor [label=\"yes\"];\n    verify_green -> green [label=\"no\"];\n    refactor -> verify_green [label=\"stay\\ngreen\"];\n    verify_green -> next;\n    next -> red;\n}\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n<Good>\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing\n</Good>\n\n<Bad>\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code\n</Bad>\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n<Good>\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass\n</Good>\n\n<Bad>\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered\n</Bad>\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\" ≠ comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after ≠ TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc ≠ systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, avoid common pitfalls:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n\n## Final Rule\n\n```\nProduction code → test exists and failed first\nOtherwise → not TDD\n```\n\nNo exceptions without your human partner's permission.",
  },
  {
    name: "using-git-worktrees",
    description: "Use when starting implementation work that requires isolation from the current branch and reproducible task execution.",
    template: "# Using Git Worktrees\n\n## Overview\n\nIn Warcraft, use worktree lifecycle tools rather than manual git worktree commands.\n\nPrimary interfaces:\n- `warcraft_worktree_create`\n- `warcraft_worktree_commit`\n- `warcraft_worktree_discard`\n- `warcraft_merge`\n\n## Standard Flow\n\n1. Confirm task is runnable (`warcraft_status`)\n2. Create isolated worktree (`warcraft_worktree_create`)\n3. Implement and verify inside that worktree\n4. Commit/report (`warcraft_worktree_commit`)\n5. Merge or discard based on outcome\n\n## Why this matters\n\nWarcraft tools keep task state, reports, and branch/worktree metadata in sync. Manual git-only operations can desynchronize artifacts.\n\n## Red Flags\n\nNever:\n- Start task work without a worktree\n- Bypass Warcraft task status updates\n- Merge/discard without reflecting result in Warcraft flow\n\nAlways:\n- Validate runnable status first\n- Capture verification evidence before commit\n- Use merge/discard paths deliberately",
  },
  {
    name: "verification-before-completion",
    description: "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    template: "# Verification Before Completion\n\n## Overview\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n**Violating the letter of this rule is violating the spirit of this rule.**\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n## The Gate Function\n\n```\nBEFORE claiming any status or expressing satisfaction:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n## Common Failures\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test command output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check, extrapolation |\n| Build succeeds | Build command: exit 0 | Linter passing, logs look good |\n| Bug fixed | Test original symptom: passes | Code changed, assumed fixed |\n| Regression test works | Red-green cycle verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\", \"Done!\", etc.)\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- Tired and wanting work over\n- **ANY wording implying success without having run verification**\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence ≠ evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter ≠ compiler |\n| \"Agent said success\" | Verify independently |\n| \"I'm tired\" | Exhaustion ≠ excuse |\n| \"Partial check is enough\" | Partial proves nothing |\n| \"Different words so rule doesn't apply\" | Spirit over letter |\n\n## Key Patterns\n\n**Tests:**\n```\n✅ [Run test command] [See: 34/34 pass] \"All tests pass\"\n❌ \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\n✅ Write → Run (pass) → Revert fix → Run (MUST FAIL) → Restore → Run (pass)\n❌ \"I've written a regression test\" (without red-green verification)\n```\n\n**Build:**\n```\n✅ [Run build] [See: exit 0] \"Build passes\"\n❌ \"Linter passed\" (linter doesn't check compilation)\n```\n\n**Requirements:**\n```\n✅ Re-read plan → Create checklist → Verify each → Report gaps or completion\n❌ \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\n✅ Agent reports success → Check VCS diff → Verify changes → Report actual state\n❌ Trust agent report\n```\n\n## Why This Matters\n\nFrom 24 failure memories:\n- your human partner said \"I don't believe you\" - trust broken\n- Undefined functions shipped - would crash\n- Missing requirements shipped - incomplete features\n- Time wasted on false completion → redirect → rework\n- Violates: \"Honesty is a core value. If you lie, you'll be replaced.\"\n\n## When To Apply\n\n**ALWAYS before:**\n- ANY variation of success/completion claims\n- ANY expression of satisfaction\n- ANY positive statement about work state\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n**Rule applies to:**\n- Exact phrases\n- Paraphrases and synonyms\n- Implications of success\n- ANY communication suggesting completion/correctness\n\n## The Bottom Line\n\n**No shortcuts for verification.**\n\nRun the command. Read the output. THEN claim the result.\n\nThis is non-negotiable.",
  },
  {
    name: "warcraft",
    description: "Plan-first AI development with isolated git worktrees and human review. Use for any feature development.",
    template: "# Warcraft Workflow\n\nPlan-first development: Plan → Approve → Execute.\n\n## Agent Modes\n\nWarcraft has two agent modes, configured via `agentMode` in `~/.config/opencode/opencode_warcraft.json`:\n\n### Unified Mode (default)\n\n```\nKhadgar (Hybrid) — plans AND orchestrates\n  ├→ Brann (Explorer/Researcher)\n  ├→ Mekkatorque (Worker/Coder)\n  └→ Algalon (Consultant/Reviewer)\n```\n\n| Agent          | Mode     | Use                                |\n| -------------- | -------- | ---------------------------------- |\n| `@khadgar`     | Primary  | Discovery + planning + orchestration |\n| `@brann`       | Subagent | Exploration/research/retrieval     |\n| `@mekkatorque` | Subagent | Executes tasks in worktrees        |\n| `@algalon`     | Subagent | Plan/code quality review           |\n\n### Dedicated Mode\n\n```\nMimiron (Planner) → Saurfang (Orchestrator)\n                         ├→ Brann (Explorer/Researcher)\n                         ├→ Mekkatorque (Worker/Coder)\n                         └→ Algalon (Consultant/Reviewer)\n```\n\n| Agent          | Mode     | Use                                |\n| -------------- | -------- | ---------------------------------- |\n| `@mimiron`     | Primary  | Discovery + planning (never executes) |\n| `@saurfang`    | Primary  | Orchestration (delegates, verifies, merges) |\n| `@brann`       | Subagent | Exploration/research/retrieval     |\n| `@mekkatorque` | Subagent | Executes tasks in worktrees        |\n| `@algalon`     | Subagent | Plan/code quality review           |\n\n---\n\n## Research Delegation (MCP + Skill + Parallel Exploration)\n\nUse MCP tools for focused research; for multi-domain exploration, use parallel Brann fan-out.\n\n| Tool                        | Use For                                |\n| --------------------------- | -------------------------------------- |\n| `grep_app_searchGitHub`     | Find code in OSS repos                 |\n| `context7_query-docs`       | Library documentation                  |\n| `websearch_web_search_exa`  | Web search and scraping                |\n| `warcraft_skill(\"ast-grep\")` | AST-aware search workflow              |\n| `task()`                    | Parallel exploration via Brann fan-out |\n\nFor exploratory fan-out, load `warcraft_skill(\"parallel-exploration\")` for the full playbook.\n\n---\n\n## Intent Classification (Do First)\n\n| Intent         | Signals                   | Action                                     |\n| -------------- | ------------------------- | ------------------------------------------ |\n| **Trivial**    | Single file, <10 lines    | Do directly. No feature.                   |\n| **Simple**     | 1-2 files, <30 min        | Quick questions → light plan or just do it |\n| **Complex**    | 3+ files, needs review    | Full feature workflow                      |\n| **Refactor**   | \"refactor\", existing code | Safety: tests, rollback, blast radius      |\n| **Greenfield** | New feature, \"build new\"  | Discovery: find patterns first             |\n\n**Don't over-plan trivial tasks.**\n\n---\n\n## Lifecycle\n\n```\nClassify Intent → Discovery → Plan → Review → Execute → Merge\n                      ↑                           │\n                      └───────── replan ──────────┘\n```\n\n---\n\n## Phase 1: Discovery\n\n### Research First (Greenfield/Complex)\n\nFor parallel exploration, load `warcraft_skill(\"parallel-exploration\")`.\n\n### Question Tool\n\n```json\n{\n  \"questions\": [\n    {\n      \"question\": \"What authentication should we use?\",\n      \"header\": \"Auth Strategy\",\n      \"options\": [\n        { \"label\": \"JWT\", \"description\": \"Token-based, stateless\" },\n        { \"label\": \"Session\", \"description\": \"Cookie-based, server state\" }\n      ]\n    }\n  ]\n}\n```\n\n### Self-Clearance Check\n\nAfter each exchange:\n\n```\n□ Core objective clear?\n□ Scope boundaries defined?\n□ No critical ambiguities?\n□ Technical approach decided?\n\nALL YES → Write plan\nANY NO → Ask the unclear thing\n```\n\n---\n\n## Phase 2: Plan\n\n### Create Feature\n\n```\nwarcraft_feature_create({ name: \"feature-name\" })\n```\n\n### Save Context\n\n```\nwarcraft_context_write({\n  name: \"research\",\n  content: \"# Findings\\n- Pattern at src/lib/auth:45-78...\"\n})\n```\n\n### Write Plan\n\n```\nwarcraft_plan_write({ content: \"...\" })\n```\n\n### Plan Structure (REQUIRED)\n\n```markdown\n# {Feature Title}\n\n## Discovery\n\n### Original Request\n\n- \"{User's exact words}\"\n\n### Interview Summary\n\n- {Point}: {Decision}\n\n### Research Findings\n\n- `{file:lines}`: {Finding}\n\n---\n\n## Non-Goals (What we're NOT building)\n\n- {Explicit exclusion}\n\n## Ghost Diffs (Alternatives Rejected)\n\n- {Approach}: {Why rejected}\n\n---\n\n## Tasks\n\n### 1. {Task Title}\n\n**Depends on**: none\n\n**What to do**:\n\n- {Implementation step}\n\n**Must NOT do**:\n\n- {Task guardrail}\n\n**References**:\n\n- `{file:lines}` — {WHY this reference}\n\n**Acceptance Criteria**:\n\n- [ ] {Verifiable outcome}\n- [ ] Run: `{command}` → {expected}\n\n---\n\n## Success Criteria\n\n- [ ] {Final checklist}\n```\n\n### Key Sections\n\n| Section                 | Purpose                                  |\n| ----------------------- | ---------------------------------------- |\n| **Discovery**           | Ground plan in user words + research     |\n| **Non-Goals**           | Prevents scope creep                     |\n| **Ghost Diffs**         | Prevents re-proposing rejected solutions |\n| **References**          | File:line citations with WHY             |\n| **Must NOT do**         | Task-level guardrails                    |\n| **Acceptance Criteria** | Verifiable conditions                    |\n| **Depends on**          | Task execution order (optional)          |\n\n### Task Dependencies\n\nThe `**Depends on**:` annotation declares which tasks must complete before a task can start.\n\n| Syntax                 | Meaning                                              |\n| ---------------------- | ---------------------------------------------------- |\n| `**Depends on**: none` | No dependencies — can run immediately or in parallel |\n| `**Depends on**: 1`    | Depends on task 1                                    |\n| `**Depends on**: 1, 3` | Depends on tasks 1 and 3                             |\n| _(omitted)_            | Implicit sequential — depends on previous task (N-1) |\n\n**Default behavior**: When no `**Depends on**:` annotation is present, the task implicitly depends on the previous task (task N depends on task N-1). This preserves backwards compatibility with existing plans.\n\n**Example**:\n\n```markdown\n### 1. Set up database schema\n\n**Depends on**: none\n...\n\n### 2. Create API endpoints\n\n**Depends on**: 1\n...\n\n### 3. Add authentication\n\n**Depends on**: 1\n...\n\n### 4. Build UI components\n\n**Depends on**: 2, 3\n...\n```\n\nIn this example, tasks 2 and 3 can run in parallel (both only depend on 1), while task 4 waits for both.\n\n---\n\n## Phase 3: Review\n\n1. User reviews `plan.md`\n2. Check comments: `warcraft_plan_read()`\n3. Revise if needed\n4. User approves via: `warcraft_plan_approve()`\n\n---\n\n## Phase 4: Execute\n\n### Sync Tasks\n\n```\nwarcraft_tasks_sync()\n```\n\n### Single Task Execution\n\n`warcraft_worktree_create` creates the worktree and returns delegation instructions with `delegationRequired: true` and a `taskToolCall` object:\n\n```\nwarcraft_worktree_create({ task: \"01-task-name\" })\n  → returns { delegationRequired: true, taskToolCall: { subagent_type, description, prompt } }\n\ntask({\n  subagent_type: \"mekkatorque\",\n  description: \"Warcraft: 01-task-name\",\n  prompt: <from taskToolCall.prompt>\n})\n  → [Mekkatorque implements in worktree]\n\nwarcraft_worktree_commit({ task: \"01-task-name\", summary: \"...\", status: \"completed\" })\n  → warcraft_merge({ task: \"01-task-name\", strategy: \"squash\" })\n```\n\n### Parallel Batch Execution\n\nUse `warcraft_batch_execute` to dispatch multiple independent tasks:\n\n1. `warcraft_batch_execute({ mode: \"preview\" })` — See runnable tasks\n2. Present to user, confirm which to run\n3. `warcraft_batch_execute({ mode: \"execute\", tasks: [\"01-...\", \"02-...\"] })` — Dispatch\n4. Issue ALL returned `task()` calls in the SAME assistant message for true parallelism\n5. After all workers return, call `warcraft_status()` and repeat for the next batch\n\nWhen multiple tasks are runnable, ask the operator:\n\n```json\n{\n  \"questions\": [\n    {\n      \"question\": \"Multiple tasks are ready. How should we proceed?\",\n      \"header\": \"Parallel Execution\",\n      \"options\": [\n        {\n          \"label\": \"Parallel\",\n          \"description\": \"Run all ready tasks simultaneously\"\n        },\n        {\n          \"label\": \"Sequential\",\n          \"description\": \"Run one at a time for easier review\"\n        },\n        { \"label\": \"Pick\", \"description\": \"Let me choose which to run\" }\n      ]\n    }\n  ]\n}\n```\n\n---\n\n## Blocker Handling\n\nWhen worker returns `status: 'blocked'`:\n\n### Quick Decision (No Plan Change)\n\n1. `warcraft_status()` - get details\n2. Ask user via question tool\n3. Resume: `warcraft_worktree_create({ task, continueFrom: \"blocked\", decision: \"...\" })`\n\n### Plan Gap Detected\n\nIf blocker suggests plan is incomplete:\n\n```json\n{\n  \"questions\": [\n    {\n      \"question\": \"This suggests our plan may need revision. How proceed?\",\n      \"header\": \"Plan Gap Detected\",\n      \"options\": [\n        { \"label\": \"Revise Plan\", \"description\": \"Go back to planning\" },\n        { \"label\": \"Quick Fix\", \"description\": \"Handle as one-off\" },\n        { \"label\": \"Abort Feature\", \"description\": \"Stop entirely\" }\n      ]\n    }\n  ]\n}\n```\n\nIf \"Revise Plan\":\n\n1. `warcraft_worktree_discard({ task })`\n2. `warcraft_context_write({ name: \"learnings\", content: \"...\" })`\n3. `warcraft_plan_write({ content: \"...\" })` (updated plan)\n4. Wait for re-approval\n\n---\n\n## Tool Reference\n\n| Phase     | Tool                                                       | Purpose                                    |\n| --------- | ---------------------------------------------------------- | ------------------------------------------ |\n| Discovery | `grep_app_searchGitHub` / `context7_query-docs` / `task()` | Research delegation (parallel exploration) |\n| Plan      | `warcraft_feature_create`                                  | Start feature                              |\n| Plan      | `warcraft_context_write`                                   | Save research                              |\n| Plan      | `warcraft_plan_write`                                      | Write plan                                 |\n| Plan      | `warcraft_plan_read`                                       | Check comments                             |\n| Plan      | `warcraft_plan_approve`                                    | Approve plan                               |\n| Execute   | `warcraft_tasks_sync`                                      | Generate tasks                             |\n| Execute   | `warcraft_task_create`                                     | Create ad-hoc task                         |\n| Execute   | `warcraft_task_update`                                     | Update task status                         |\n| Execute   | `warcraft_worktree_create`                                 | Create worktree + delegation instructions  |\n| Execute   | `warcraft_worktree_commit`                                 | Finish task                                |\n| Execute   | `warcraft_worktree_discard`                                | Discard task                               |\n| Execute   | `warcraft_merge`                                           | Integrate task branch                      |\n| Execute   | `warcraft_batch_execute`                                   | Parallel task dispatch                     |\n| Execute   | `warcraft_status`                                          | Check workers/blockers                     |\n| Complete  | `warcraft_feature_complete`                                | Mark done                                  |\n| Any       | `warcraft_skill`                                           | Load skill on demand                       |\n| Any       | `warcraft_agents_md`                                       | Sync/init AGENTS.md                        |\n\n---\n\n## Iron Laws\n\n**Never:**\n\n- Plan without discovery\n- Execute without approval\n- Complete without verification\n- Assume when uncertain - ASK\n- Force through blockers that suggest plan gaps\n\n**Always:**\n\n- Match effort to complexity\n- Include file:line references with WHY\n- Define Non-Goals and Must NOT guardrails\n- Provide verification commands\n- Offer replan when blockers suggest gaps\n\n---\n\n## Error Recovery\n\n### Task Failed\n\n```\nwarcraft_worktree_discard({ task })  # Discard\nwarcraft_worktree_create({ task })   # Fresh start\n```\n\n### After 3 Failures\n\n1. Stop all workers\n2. Delegate to Algalon: `task({ subagent_type: \"algalon\", prompt: \"Analyze failure pattern...\" })`\n3. Ask user how to proceed\n\n### Merge Conflicts\n\n1. Resolve in worktree\n2. Commit resolution\n3. `warcraft_merge` again",
  },
  {
    name: "writing-plans",
    description: "Use when you have a spec or requirements for a multi-step task, before touching code",
    template: "# Writing Plans\n\n## Overview\n\nWrite comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.\n\nAssume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.\n\n**Announce at start:** \"I'm using the writing-plans skill to create the implementation plan.\"\n\n**Context:** Planning is read-only. Use `warcraft_feature_create` + `warcraft_plan_write` and avoid worktrees during planning.\n\n**Save plans to:** `warcraft_plan_write` (writes to `<warcraft-root>/<feature>/plan.md`, where warcraft-root is `.beads/artifacts` in `on` mode and `docs` in `off` mode)\n\n## Bite-Sized Task Granularity\n\n**Each step is one action (2-5 minutes):**\n- \"Write the failing test\" - step\n- \"Run it to make sure it fails\" - step\n- \"Implement the minimal code to make the test pass\" - step\n- \"Run the tests and make sure they pass\" - step\n- \"Commit\" - step\n\n## Plan Structure\n\n**Every plan MUST follow this structure:**\n\n````markdown\n# [Feature Name]\n\n## Discovery\n\n### Original Request\n- \"{User's exact words}\"\n\n### Interview Summary\n- {Point}: {Decision}\n\n### Research Findings\n- `{file:lines}`: {Finding}\n\n---\n\n## Non-Goals (What we're NOT building)\n- {Explicit exclusion}\n\n---\n\n## Tasks\n\n### 1. Task Name\n\nUse the Task Structure template below for every task.\n````\n\n\n## Task Structure\n\nThe **Depends on** annotation declares task execution order:\n- **Depends on**: none — No dependencies; can run immediately or in parallel\n- **Depends on**: 1 — Depends on task 1\n- **Depends on**: 1, 3 — Depends on tasks 1 and 3\n\nAlways include **Depends on** for each task. Use `none` to enable parallel starts.\n\n````markdown\n### N. Task Name\n\n**Depends on**: none\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**What to do**:\n- Step 1: Write the failing test\n  ```python\n  def test_specific_behavior():\n      result = function(input)\n      assert result == expected\n  ```\n- Step 2: Run test to verify it fails\n  - Run: `pytest tests/path/test.py::test_name -v`\n  - Expected: FAIL with \"function not defined\"\n- Step 3: Write minimal implementation\n  ```python\n  def function(input):\n      return expected\n  ```\n- Step 4: Run test to verify it passes\n  - Run: `pytest tests/path/test.py::test_name -v`\n  - Expected: PASS\n- Step 5: Commit\n  ```bash\n  git add tests/path/test.py src/path/file.py\n  git commit -m \"feat: add specific feature\"\n  ```\n\n**Must NOT do**:\n- {Task guardrail}\n\n**References**:\n- `{file:lines}` — {Why this reference matters}\n\n**Verify**:\n- [ ] Run: `{command}` → {expected}\n- [ ] {Additional acceptance criteria}\n\nAll verification MUST be agent-executable (no human intervention):\n✅ `bun test` → all pass\n✅ `curl -X POST /api/x` → 201\n❌ \"User manually tests...\"\n❌ \"Visually confirm...\"\n````\n\n## Remember\n- Exact file paths always\n- Complete code in plan (not \"add validation\")\n- Exact commands with expected output\n- Reference relevant skills with @ syntax\n- DRY, YAGNI, TDD, frequent commits\n- All acceptance criteria must be agent-executable (zero human intervention)\n\n## Execution Handoff\n\nAfter saving the plan, ask whether to consult Hygienic (Consultant/Reviewer/Debugger) before offering execution choice.\n\nPlan complete and saved to `<warcraft-root>/<feature>/plan.md`.\n\n\nTwo execution options:\n1. Subagent-Driven (this session) - I dispatch fresh subagent per task, review between tasks, fast iteration\n2. Parallel Session (separate) - Open new session with executing-plans, batch execution with checkpoints\n\nWhich approach?\n\n**If Subagent-Driven chosen:**\n- Stay in this session\n- Fresh subagent per task + code review\n\n**If Parallel Session chosen:**\n- Guide them to open new session in worktree\n- **REQUIRED SUB-SKILL:** New session uses warcraft_skill:executing-plans",
  },
  {
    name: "writing-skills",
    description: "Use when creating or updating built-in skills so they remain discoverable, testable, and aligned with both upstream superpowers and Warcraft workflows.",
    template: "# Writing Skills\n\n## Overview\n\nA skill exists to produce repeatable behavior in repeatable situations.\n\nIn this repository, skill sources live at:\n- `packages/opencode-warcraft/skills/<skill-id>/SKILL.md`\n\nGenerated registry target:\n- `packages/opencode-warcraft/src/skills/registry.generated.ts`\n\n## Authoring Rules\n\n1. Use valid frontmatter:\n   - `name`: kebab-case skill id\n   - `description`: starts with `Use when ...` and describes trigger conditions only\n\n2. Keep content actionable:\n   - clear trigger conditions\n   - deterministic process/checklist\n   - red flags and common mistakes\n\n3. Align with Warcraft behavior:\n   - prefer `warcraft_*` tools for workflow operations\n   - keep instructions compatible with this repo's plan-first guardrails\n\n## Upgrading from Upstream Superpowers\n\nWhen refreshing skills from `obra/superpowers`:\n\n1. Compare upstream skill directories with local `skills/`.\n2. Sync overlapping skill content where it improves guidance.\n3. Preserve Warcraft-specific behavior in workflow skills (for example: `warcraft_*` tool usage, runnable-task logic, worktree lifecycle).\n4. Keep project-only skills intact (`warcraft`, `parallel-exploration`, `agents-md-mastery`, etc.).\n5. Re-run generation and tests before handoff.\n\nRule of thumb: upstream provides process quality; local overrides provide project fit.\n\n## Validation Workflow\n\nAfter adding/updating skills:\n\n1. Regenerate registry:\n   - `bun run --filter opencode-warcraft generate-skills`\n2. Run targeted tests:\n   - `bun test src/skills`\n\nIf tests fail, fix skill content or metadata and regenerate.\n\n## Red Flags\n\nNever:\n- Add vague descriptions that do not signal clear trigger conditions\n- Copy upstream instructions that reference unavailable tools/workflows\n- Edit `registry.generated.ts` manually\n\nAlways:\n- Keep skill names unique\n- Regenerate registry after skill edits\n- Verify through tests before handoff",
  }
];
